{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable \n",
    "import copy\n",
    "import random\n",
    "import seaborn as sns\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "torch.cuda.manual_seed(666)\n",
    "np.random.seed(666)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('final_df.csv').groupby(['SUBJECT_ID','ICUSTAY_ID','HADM_ID', 'rel_stay_diff','read_binary_90day']).agg(\n",
    "#     lambda x: '\\n '.join(x)).reset_index().to_csv('final_concat_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 'all_rows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_concat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['read_binary_90day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ICUSTAY_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data loader with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dead patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = pd.read_csv('../physionet.org/files/mimiciii/1.4/PATIENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stays = pd.read_csv('../physionet.org/files/mimiciii/1.4/ICUSTAYS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay = stays.merge(pat, on='SUBJECT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay['OUTTIME'] = pd.to_datetime(exclude_stay['OUTTIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay['INTIME'] = pd.to_datetime(exclude_stay['INTIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay['DOB'] = pd.to_datetime(exclude_stay['DOB'])\n",
    "exclude_stay['DOD'] = pd.to_datetime(exclude_stay['DOD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay['90day_after_out'] = exclude_stay['OUTTIME'] + timedelta(days=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay = exclude_stay[(~((exclude_stay['DOD'] > exclude_stay['INTIME']) & \n",
    "             (exclude_stay['DOD'] <= exclude_stay['90day_after_out']))) |\n",
    "              (df['read_binary_90day'] ==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# did not die before ICU stay started\n",
    "exclude_stay = exclude_stay[~(exclude_stay['DOD'] < exclude_stay['INTIME'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # was not born within the 10 days prior to intime \n",
    "# exclude_stay = exclude_stay[~(exclude_stay['DOB'] >= exclude_stay['INTIME'] - timedelta(days=10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_stay[exclude_stay['ROW_ID_x'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mort_check = df.merge(exclude_stay[['ROW_ID_x', 'ICUSTAY_ID']], how='left', on = 'ICUSTAY_ID' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mort_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mort_check[~mort_check['ROW_ID_x'].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mort_check[~mort_check['ROW_ID_x'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create even assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['read_binary_90day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = df['read_binary_90day'].value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df[df['read_binary_90day'] == 0]\n",
    "df_class_1 = df[df['read_binary_90day'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_class_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1, random_state=666)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_under['read_binary_90day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_under.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_test_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('downsampled_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TEXT'].apply(lambda x: len(x)).describe([.75,.8,.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TEXT'].apply(lambda x: len(x)).describe([.75,.8,.9])[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_max = int(df['TEXT'].apply(lambda x: len(x)).describe([.75,.8,.9])[-2])\n",
    "# this will cover 90% of all notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['TEXT'].apply(lambda x: x[:sentence_max]  if len(x) > sentence_max else x).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['read_binary_90day'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the model we want to use\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# We need to create the model and tokenizer\n",
    "# model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[3])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[3]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Tokenize all of the sentences and map the tokens to thier word IDs. \n",
    "# Did this in the tokenizer notebook to take advantage of multiple machines\n",
    "# input_ids = []\n",
    "# attention_masks = []\n",
    "\n",
    "# LOG_EVERY_N = 100\n",
    "# counting = 0\n",
    "# # For every sentence...\n",
    "# for sent in sentences:\n",
    "#     # `encode_plus` will:\n",
    "#     #   (1) Tokenize the sentence.\n",
    "#     #   (2) Prepend the `[CLS]` token to the start.\n",
    "#     #   (3) Append the `[SEP]` token to the end.\n",
    "#     #   (4) Map tokens to their IDs.\n",
    "#     #   (5) Pad or truncate the sentence to `max_length`\n",
    "#     #   (6) Create attention masks for [PAD] tokens.\n",
    "#     encoded_dict = tokenizer.encode_plus(\n",
    "#                         sent,                      # Sentence to encode.\n",
    "#                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#                         max_length = 512,           # Pad & truncate all sentences.\n",
    "#                         pad_to_max_length = True,\n",
    "#                         return_attention_mask = True,   # Construct attn. masks.\n",
    "#                         return_tensors = 'pt',     # Return pytorch tensors.\n",
    "#                    )\n",
    "    \n",
    "#     # Add the encoded sentence to the list.    \n",
    "#     input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "#     # And its attention mask (simply differentiates padding from non-padding).\n",
    "#     attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "#     if (counting % LOG_EVERY_N) == 0:\n",
    "#         print (f'logging: ...{str(counting)} sentences at {datetime.now()}')\n",
    "# #     if (counting % LOG_EVERY_N*10) == 0:\n",
    "# #         file_out = open(f'input_ids_sentences_{str(counting)}.pk', \"wb\")\n",
    "# #         pickle.dump(input_ids, file_out)\n",
    "# #         file_out = open(f'attention_masks_sentences_{str(counting)}.pk', \"wb\")\n",
    "# #         pickle.dump(attention_masks, file_out)\n",
    "        \n",
    "#     counting+=1\n",
    "\n",
    "# # Convert the lists into tensors.\n",
    "# input_ids = torch.cat(input_ids, dim=0)\n",
    "# attention_masks = torch.cat(attention_masks, dim=0)\n",
    "# labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_out = open(f'input_ids_nrows_{str(nrows)}.pk', \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(input_ids, file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_out = open(f'attention_masks_nrows_{str(nrows)}.pk', \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(attention_masks, file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token_output/input_ids_sentences_14800.pk', 'rb') as pickle_file:\n",
    "    input_ids_test = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token_output/input_ids_token_processing_sentences_14800_15000.pk', 'rb') as pickle_file:\n",
    "    input_ids_test2 = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test[-2] == input_ids_test2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overlaps by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_input_ids = input_ids_test[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_input_ids = master_input_ids + input_ids_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(15000, 61134, step):\n",
    "    print (f'batch starting row {x}')\n",
    "    if x == 61000:\n",
    "        nrows= 61134 - 61000 + 1\n",
    "    else:\n",
    "        nrows= step\n",
    "    with open(f'token_output/input_ids_token_processing_sentences_{str(x)}_{str(x+nrows)}.pk', 'rb') as pickle_file:\n",
    "        master_input_ids+=pickle.load(pickle_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token_output/attention_masks_sentences_14700.pk', 'rb') as pickle_file:\n",
    "    master_attention_masks = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token_output/attention_masks_token_processing_sentences_14700_15000.pk', 'rb') as pickle_file:\n",
    "    attention_masks_test = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_attention_masks[-2] == attention_masks_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_attention_masks = master_attention_masks[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_attention_masks+=attention_masks_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(15000, 61134, step):\n",
    "    print (f'batch starting row {x}')\n",
    "    if x == 61000:\n",
    "        nrows= 61134 - 61000 + 1\n",
    "    else:\n",
    "        nrows= step\n",
    "    with open(f'token_output/attention_masks_token_processing_sentences_{str(x)}_{str(x+nrows)}.pk', 'rb') as pickle_file:\n",
    "        master_attention_masks+=pickle.load(pickle_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = open(f'master_attention_masks.pk', \"wb\")\n",
    "pickle.dump(master_attention_masks, file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = open(f'master_input_ids.pk', \"wb\")\n",
    "pickle.dump(master_input_ids, file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_keep_mort_check = mort_check[~mort_check['ROW_ID_x'].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_keep_mort_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([master_input_ids[i] for i in index_keep_mort_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_input_ids_mort_filt = [master_input_ids[i] for i in index_keep_mort_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_attention_masks_mort_filt = [master_attention_masks[i] for i in index_keep_mort_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = open(f'master_input_ids_mort_filt.pk', \"wb\")\n",
    "pickle.dump(master_input_ids_mort_filt, file_out)\n",
    "file_out = open(f'master_attention_masks_mort_filt.pk', \"wb\")\n",
    "pickle.dump(master_attention_masks_mort_filt, file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use records that did not have mortality and applied to the balanced set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_attention_masks = [master_attention_masks[i] for i in df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_input_ids = [master_input_ids[i] for i in df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['read_binary_90day'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(master_input_ids, dim=0)\n",
    "attention_masks = torch.cat(master_attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset = TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [int(len(df)*.6)+1, int(len(df)*.2)+1, int(len(df)*.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = random_split(t_dataset, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'train':train, 'test':test, 'val': val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "for x in data_dict.keys():\n",
    "    dataloaders[x] = DataLoader(\n",
    "                data_dict[x], # The validation samples.\n",
    "                sampler = SequentialSampler(data_dict[x]), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = dict(zip(['train', 'val', 'test'], splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verified the same data sizes are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first iteration of models without balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_BERT(model, dataloaders, \n",
    "          learning_rate=2e-5,  # args.learning_rate - default is 5e-5\n",
    "               num_epoch=25\n",
    "         ):\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    acc_dict = {'train':[],'validation':[]}\n",
    "    loss_dict = {'train':[],'validation':[]}\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                )\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "                total_steps = len(dataloaders[phase]) * num_epoch\n",
    "\n",
    "                # Create the learning rate scheduler.\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "            else:\n",
    "                model.train(False)\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0 \n",
    "\n",
    "            for i, (data, b_input_mask, labels) in enumerate(dataloaders[phase]):\n",
    "                data, b_input_mask, labels = data.to(device), b_input_mask.to(device), labels.to(device)\n",
    "                (loss, outputs) = model(data, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=labels)\n",
    "                model.zero_grad()\n",
    "#                 loss is automatically output from the model. So no need to run loss separately\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "                _, preds = torch.max(outputs, dim = 1)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels).item()\n",
    "\n",
    "        # Evaluate after every epochh         \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "        \n",
    "            if phase == 'train':\n",
    "                loss_dict['train'].append(epoch_loss)\n",
    "                acc_dict['train'].append(epoch_acc)\n",
    "            else:\n",
    "                loss_dict['validation'].append(epoch_loss)\n",
    "                acc_dict['validation'].append(epoch_acc)\n",
    "                    \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, f'model/full_data_readmission_bert_epoch{epoch + 1}.pth')\n",
    "#                 scheduler.step(epoch_loss)\n",
    "\n",
    "            print('{} set | epoch: {}/{} | Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epoch + 1, num_epoch, epoch_loss, epoch_acc))  \n",
    "            time_elapsed = time.time() - start_time\n",
    "            print('Training time so far: {}minutes {}s'.format(int(time_elapsed / 60), time_elapsed % 60))\n",
    "\n",
    "            \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training time: {}minutes {}s'.format(int(time_elapsed / 60), time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    for i, phase in enumerate(['train','validation']):\n",
    "\n",
    "        fig = plt.figure()\n",
    "        \n",
    "        a = fig.add_subplot(2,2,2*i+1)\n",
    "        plt.plot(loss_dict[phase])\n",
    "        plt.title('Loss per epoch for ' + phase)\n",
    "\n",
    "        a = fig.add_subplot(2,2,2*i+2)\n",
    "        plt.plot(acc_dict[phase])\n",
    "        plt.title('Accuracy per epoch for ' + phase)\n",
    "        plt.show()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)            \n",
    "\n",
    "    # test set evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    truths = []\n",
    "    data_for_example = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, b_input_mask, labels) in enumerate(dataloaders['test']):\n",
    "            data, b_input_mask, labels = data.to(device), b_input_mask.to(device), labels.to(device)\n",
    "            (loss, outputs) = model(data, \n",
    "                   token_type_ids=None, \n",
    "                   attention_mask=b_input_mask,\n",
    "                   labels=labels)\n",
    "            model.zero_grad()\n",
    "#                 loss is automatically output from the model. So no need to run loss separately\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "            _, preds = torch.max(outputs, dim = 1)\n",
    "            predictions += list(preds.cpu().numpy())\n",
    "            truths += list(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum()\n",
    "            data_for_example.append(data)\n",
    "\n",
    "        acc = (1.0 * correct / total)\n",
    "        elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "        print('Test set | Accuracy: {:6.4f} | time elapse: {:>9}'.format(\n",
    "            acc, elapse))\n",
    "    \n",
    "    return {'Model': model, 'LossDict': loss_dict, 'AccDict': acc_dict, \n",
    "            'test_predictions': predictions, 'test_pred_prob': outputs, 'test_truths': truths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "Bestmodel = train_BERT(model, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "labels=None, sample_weight=None, normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('test set confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "labels=None, sample_weight=None, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model2 = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model2.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_BERT_2(model, dataloaders, \n",
    "          learning_rate=5e-5,  # args.learning_rate - default is 5e-5\n",
    "               num_epoch=3\n",
    "         ):\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    acc_dict = {'train':[],'validation':[]}\n",
    "    loss_dict = {'train':[],'validation':[]}\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                )\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "                total_steps = len(dataloaders[phase]) * num_epoch\n",
    "\n",
    "                # Create the learning rate scheduler.\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "            else:\n",
    "                model.train(False)\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0 \n",
    "\n",
    "            for i, (data, b_input_mask, labels) in enumerate(dataloaders[phase]):\n",
    "                data, b_input_mask, labels = data.to(device), b_input_mask.to(device), labels.to(device)\n",
    "                (loss, outputs) = model(data, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=labels)\n",
    "                model.zero_grad()\n",
    "#                 loss is automatically output from the model. So no need to run loss separately\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "                _, preds = torch.max(outputs, dim = 1)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels).item()\n",
    "\n",
    "        # Evaluate after every epochh         \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "        \n",
    "            if phase == 'train':\n",
    "                loss_dict['train'].append(epoch_loss)\n",
    "                acc_dict['train'].append(epoch_acc)\n",
    "            else:\n",
    "                loss_dict['validation'].append(epoch_loss)\n",
    "                acc_dict['validation'].append(epoch_acc)\n",
    "                    \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, f'model/full_data_read_bert_lr_{learning_rate}_numepoch_{num_epoch}_currepoch{epoch + 1}.pth')\n",
    "#                 scheduler.step(epoch_loss)\n",
    "\n",
    "            print('{} set | epoch: {}/{} | Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epoch + 1, num_epoch, epoch_loss, epoch_acc))  \n",
    "            time_elapsed = time.time() - start_time\n",
    "            print('Training time so far: {}minutes {}s'.format(int(time_elapsed / 60), time_elapsed % 60))\n",
    "\n",
    "            \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training time: {}minutes {}s'.format(int(time_elapsed / 60), time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    for i, phase in enumerate(['train','validation']):\n",
    "\n",
    "        fig = plt.figure()\n",
    "        \n",
    "        a = fig.add_subplot(2,2,2*i+1)\n",
    "        plt.plot(loss_dict[phase])\n",
    "        plt.title('Loss per epoch for ' + phase)\n",
    "\n",
    "        a = fig.add_subplot(2,2,2*i+2)\n",
    "        plt.plot(acc_dict[phase])\n",
    "        plt.title('Accuracy per epoch for ' + phase)\n",
    "        plt.show()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)            \n",
    "\n",
    "    # test set evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    truths = []\n",
    "    data_for_example = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, b_input_mask, labels) in enumerate(dataloaders['test']):\n",
    "            data, b_input_mask, labels = data.to(device), b_input_mask.to(device), labels.to(device)\n",
    "            (loss, outputs) = model(data, \n",
    "                   token_type_ids=None, \n",
    "                   attention_mask=b_input_mask,\n",
    "                   labels=labels)\n",
    "            model.zero_grad()\n",
    "#                 loss is automatically output from the model. So no need to run loss separately\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "            _, preds = torch.max(outputs, dim = 1)\n",
    "            predictions += list(preds.cpu().numpy())\n",
    "            truths += list(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum()\n",
    "            data_for_example.append(data)\n",
    "\n",
    "        acc = (1.0 * correct / total)\n",
    "        elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "        print('Test set | Accuracy: {:6.4f} | time elapse: {:>9}'.format(\n",
    "            acc, elapse))\n",
    "    \n",
    "    return {'Model': model, 'LossDict': loss_dict, 'AccDict': acc_dict, \n",
    "            'test_predictions': predictions, 'test_pred_prob': outputs, 'test_truths': truths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "Bestmodel = train_BERT_2(model2, dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "labels=None, sample_weight=None, normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('test set confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "labels=None, sample_weight=None, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "\n",
    "for learn_opt in [5e-5, 3e-5, 2e-5]:\n",
    "    for epoch_opt in [3, 4, 5, 6]:\n",
    "        print (f'learning rate of {learn_opt}, count of epochs {epoch_opt}')\n",
    "        Bestmodel = train_BERT_2(model2, dataloaders, learning_rate=learn_opt,\n",
    "               num_epoch=3)\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "                labels=None, sample_weight=None, normalize=None)\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('test set confusion matrix')\n",
    "        plt.show()\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "        labels=None, sample_weight=None, normalize='true')\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('normalized confusion matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model3 = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "\n",
    "for learn_opt in [5e-5, 3e-5, 2e-5]:\n",
    "    for epoch_opt in [4, 5, 6]:\n",
    "        print (f'learning rate of {learn_opt}, count of epochs {epoch_opt}')\n",
    "        \n",
    "        # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "        # linear classification layer on top. \n",
    "        model3 = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "\n",
    "        # Tell pytorch to run this model on the GPU.\n",
    "        model3.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        Bestmodel = train_BERT_2(model3, dataloaders, learning_rate=learn_opt,\n",
    "               num_epoch=epoch_opt)\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "                labels=None, sample_weight=None, normalize=None)\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('test set confusion matrix')\n",
    "        plt.show()\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "        labels=None, sample_weight=None, normalize='true')\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('normalized confusion matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "\n",
    "for learn_opt in [3e-5, 2e-5]:\n",
    "    for epoch_opt in [6]:\n",
    "        print (f'learning rate of {learn_opt}, count of epochs {epoch_opt}')\n",
    "        \n",
    "        # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "        # linear classification layer on top. \n",
    "        model3 = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "\n",
    "        # Tell pytorch to run this model on the GPU.\n",
    "        model3.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        Bestmodel = train_BERT_2(model3, dataloaders, learning_rate=learn_opt,\n",
    "               num_epoch=epoch_opt)\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "                labels=None, sample_weight=None, normalize=None)\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('test set confusion matrix')\n",
    "        plt.show()\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "        labels=None, sample_weight=None, normalize='true')\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('normalized confusion matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "\n",
    "for learn_opt in [3e-5]:\n",
    "    for epoch_opt in [3, 4, 5]:\n",
    "        print (f'learning rate of {learn_opt}, count of epochs {epoch_opt}')\n",
    "        \n",
    "        # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "        # linear classification layer on top. \n",
    "        model4 = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "\n",
    "        # Tell pytorch to run this model on the GPU.\n",
    "        model4.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        Bestmodel = train_BERT_2(model4, dataloaders, learning_rate=learn_opt,\n",
    "               num_epoch=epoch_opt)\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "                labels=None, sample_weight=None, normalize=None)\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('test set confusion matrix')\n",
    "        plt.show()\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "        labels=None, sample_weight=None, normalize='true')\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('normalized confusion matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "\n",
    "for learn_opt in [2e-5]:\n",
    "    for epoch_opt in [3, 4, 5]:\n",
    "        print (f'learning rate of {learn_opt}, count of epochs {epoch_opt}')\n",
    "        \n",
    "        # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "        # linear classification layer on top. \n",
    "        model4 = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "\n",
    "        # Tell pytorch to run this model on the GPU.\n",
    "        model4.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        Bestmodel = train_BERT_2(model4, dataloaders, learning_rate=learn_opt,\n",
    "               num_epoch=epoch_opt)\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "                labels=None, sample_weight=None, normalize=None)\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('test set confusion matrix')\n",
    "        plt.show()\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "        labels=None, sample_weight=None, normalize='true')\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('normalized confusion matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model_load = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model_load.cuda()\n",
    "model_load.load_state_dict(torch.load('model/full_data_read_bert_lr_2e-05_numepoch_3_currepoch2.pth'))\n",
    "model_load.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test (model):\n",
    "    # test set evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    truths = []\n",
    "    data_for_example = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, b_input_mask, labels) in enumerate(dataloaders['test']):\n",
    "            data, b_input_mask, labels = data.to(device), b_input_mask.to(device), labels.to(device)\n",
    "            (loss, outputs) = model(data, \n",
    "                   token_type_ids=None, \n",
    "                   attention_mask=b_input_mask,\n",
    "                   labels=labels)\n",
    "            model.zero_grad()\n",
    "#                 loss is automatically output from the model. So no need to run loss separately\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "            _, preds = torch.max(outputs, dim = 1)\n",
    "            predictions += list(preds.cpu().numpy())\n",
    "            truths += list(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum()\n",
    "            data_for_example.append(data)\n",
    "\n",
    "        acc = (1.0 * correct / total)\n",
    "#         elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "        print('Test set | Accuracy: {:6.4f}'.format(\n",
    "            acc))\n",
    "\n",
    "    return {'test_predictions': predictions, 'test_truths': truths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bestmodel_preds = model_test(model_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulled most successful model back in to create some of the output visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(Bestmodel_preds['test_truths'], Bestmodel_preds['test_predictions'],\n",
    "                labels=None, sample_weight=None, normalize=None)\n",
    "print (confusion)\n",
    "df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('test set confusion matrix')\n",
    "plt.show()\n",
    "confusion = confusion_matrix(Bestmodel_preds['test_truths'], Bestmodel_preds['test_predictions'],\n",
    "labels=None, sample_weight=None, normalize='true')\n",
    "print (confusion)\n",
    "df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(Bestmodel_preds['test_truths'], Bestmodel_preds['test_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "\n",
    "for learn_opt in [2e-5]:\n",
    "    for epoch_opt in [15]:\n",
    "        print (f'learning rate of {learn_opt}, count of epochs {epoch_opt}')\n",
    "        \n",
    "        # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "        # linear classification layer on top. \n",
    "        model4 = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "\n",
    "        # Tell pytorch to run this model on the GPU.\n",
    "        model4.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        Bestmodel = train_BERT_2(model4, dataloaders, learning_rate=learn_opt,\n",
    "               num_epoch=epoch_opt)\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "                labels=None, sample_weight=None, normalize=None)\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('test set confusion matrix')\n",
    "        plt.show()\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "        labels=None, sample_weight=None, normalize='true')\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('normalized confusion matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models trained with balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_BERT_3(model, dataloaders, \n",
    "          learning_rate=5e-5,  # args.learning_rate - default is 5e-5\n",
    "               num_epoch=3\n",
    "         ):\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    acc_dict = {'train':[],'validation':[]}\n",
    "    loss_dict = {'train':[],'validation':[]}\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
    "                )\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "                total_steps = len(dataloaders[phase]) * num_epoch\n",
    "\n",
    "                # Create the learning rate scheduler.\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "            else:\n",
    "                model.train(False)\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0 \n",
    "\n",
    "            for i, (data, b_input_mask, labels) in enumerate(dataloaders[phase]):\n",
    "                data, b_input_mask, labels = data.to(device), b_input_mask.to(device), labels.to(device)\n",
    "                (loss, outputs) = model(data, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=labels)\n",
    "                model.zero_grad()\n",
    "#                 loss is automatically output from the model. So no need to run loss separately\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "                _, preds = torch.max(outputs, dim = 1)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels).item()\n",
    "\n",
    "        # Evaluate after every epochh         \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "        \n",
    "            if phase == 'train':\n",
    "                loss_dict['train'].append(epoch_loss)\n",
    "                acc_dict['train'].append(epoch_acc)\n",
    "            else:\n",
    "                loss_dict['validation'].append(epoch_loss)\n",
    "                acc_dict['validation'].append(epoch_acc)\n",
    "                    \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, f'model/full_data_read_bert_lr_{learning_rate}_numepoch_{num_epoch}_currepoch{epoch + 1}.pth')\n",
    "#                 scheduler.step(epoch_loss)\n",
    "\n",
    "            print('{} set | epoch: {}/{} | Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epoch + 1, num_epoch, epoch_loss, epoch_acc))  \n",
    "            time_elapsed = time.time() - start_time\n",
    "            print('Training time so far: {}minutes {}s'.format(int(time_elapsed / 60), time_elapsed % 60))\n",
    "\n",
    "            \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training time: {}minutes {}s'.format(int(time_elapsed / 60), time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    for i, phase in enumerate(['train','validation']):\n",
    "\n",
    "        fig = plt.figure()\n",
    "        \n",
    "        a = fig.add_subplot(2,2,2*i+1)\n",
    "        plt.plot(loss_dict[phase])\n",
    "        plt.title('Loss per epoch for ' + phase)\n",
    "\n",
    "        a = fig.add_subplot(2,2,2*i+2)\n",
    "        plt.plot(acc_dict[phase])\n",
    "        plt.title('Accuracy per epoch for ' + phase)\n",
    "        plt.show()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)            \n",
    "\n",
    "    # test set evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    truths = []\n",
    "    pred_prob_lst = []\n",
    "    data_for_example = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, b_input_mask, labels) in enumerate(dataloaders['test']):\n",
    "            data, b_input_mask, labels = data.to(device), b_input_mask.to(device), labels.to(device)\n",
    "            (loss, outputs) = model(data, \n",
    "                   token_type_ids=None, \n",
    "                   attention_mask=b_input_mask,\n",
    "                   labels=labels)\n",
    "            model.zero_grad()\n",
    "#                 loss is automatically output from the model. So no need to run loss separately\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "            pred_prob, preds = torch.max(outputs, dim = 1)\n",
    "            predictions += list(preds.cpu().numpy())\n",
    "            pred_prob_lst += list(pred_prob.cpu().numpy())\n",
    "            truths += list(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum()\n",
    "            data_for_example.append(data)\n",
    "\n",
    "        acc = (1.0 * correct / total)\n",
    "        elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "        print('Test set | Accuracy: {:6.4f} | time elapse: {:>9}'.format(\n",
    "            acc, elapse))\n",
    "    \n",
    "    return {'Model': model, 'LossDict': loss_dict, 'AccDict': acc_dict, \n",
    "            'test_predictions': predictions, 'test_pred_prob': pred_prob_lst, 'test_truths': truths}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "\n",
    "for learn_opt in [2e-5]:\n",
    "    for epoch_opt in [6]:\n",
    "        print (f'learning rate of {learn_opt}, count of epochs {epoch_opt}')\n",
    "        \n",
    "        # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "        # linear classification layer on top. \n",
    "        model5 = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "\n",
    "        # Tell pytorch to run this model on the GPU.\n",
    "        model5.cuda()\n",
    "        \n",
    "        \n",
    "        Bestmodel = train_BERT_3(model5, dataloaders, learning_rate=learn_opt,\n",
    "               num_epoch=epoch_opt)\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "                labels=None, sample_weight=None, normalize=None)\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('test set confusion matrix')\n",
    "        plt.show()\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "        labels=None, sample_weight=None, normalize='true')\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('normalized confusion matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "\n",
    "for learn_opt in [2e-5]:\n",
    "    for epoch_opt in [5]:\n",
    "        print (f'learning rate of {learn_opt}, count of epochs {epoch_opt}')\n",
    "        \n",
    "        # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "        # linear classification layer on top. \n",
    "        model5 = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "\n",
    "        # Tell pytorch to run this model on the GPU.\n",
    "        model5.cuda()\n",
    "        \n",
    "        \n",
    "        Bestmodel = train_BERT_3(model5, dataloaders, learning_rate=learn_opt,\n",
    "               num_epoch=epoch_opt)\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "                labels=None, sample_weight=None, normalize=None)\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('test set confusion matrix')\n",
    "        plt.show()\n",
    "        confusion = confusion_matrix(Bestmodel['test_truths'], Bestmodel['test_predictions'],\n",
    "        labels=None, sample_weight=None, normalize='true')\n",
    "        print (confusion)\n",
    "        df_cm = pd.DataFrame(confusion, index=['not readmitted', 'readmitted'], columns=['not readmitted', 'readmitted'])\n",
    "        plt.figure(figsize = (10,7))\n",
    "        sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.title('normalized confusion matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model_load = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model_load.cuda()\n",
    "model_load.load_state_dict(torch.load('model/full_data_read_bert_lr_2e-05_numepoch_6_currepoch2.pth'))\n",
    "model_load.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_with_pred(model):\n",
    "# test set evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    truths = []\n",
    "    pred_prob_lst = []\n",
    "    data_for_example = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, b_input_mask, labels) in enumerate(dataloaders['test']):\n",
    "            data, b_input_mask, labels = data.to(device), b_input_mask.to(device), labels.to(device)\n",
    "            (loss, outputs) = model(data, \n",
    "                   token_type_ids=None, \n",
    "                   attention_mask=b_input_mask,\n",
    "                   labels=labels)\n",
    "            model.zero_grad()\n",
    "#                 loss is automatically output from the model. So no need to run loss separately\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "            pred_prob, preds = torch.max(outputs, dim = 1)\n",
    "            predictions += list(preds.cpu().numpy())\n",
    "            pred_prob_lst += list(pred_prob.cpu().numpy())\n",
    "            truths += list(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum()\n",
    "            data_for_example.append(data)\n",
    "\n",
    "        acc = (1.0 * correct / total)\n",
    "#         elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "        print('Test set | Accuracy: {:6.4f}'.format(\n",
    "            acc))\n",
    "    \n",
    "    return {'Model': model, \n",
    "            'test_predictions': predictions, 'test_pred_prob': pred_prob_lst, 'test_truths': truths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bestmodel_preds = test_model_with_pred(model_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(Bestmodel_preds['test_truths'], Bestmodel_preds['test_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr , thresholds = roc_curve(Bestmodel_preds['test_truths'], Bestmodel_preds['test_predictions'])\n",
    "\n",
    "plt.plot(fpr,tpr) \n",
    "plt.axis([0,1,0,1]) \n",
    "plt.xlabel('False Positive Rate') \n",
    "plt.ylabel('True Positive Rate') \n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skplt.metrics.plot_precision_recall_curve(np.array(Bestmodel_preds['test_truths']), \n",
    "                                          np.array(list(zip(Bestmodel_preds['test_pred_prob'], \n",
    "                                                    Bestmodel_preds['test_predictions']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc_curve(np.array(Bestmodel_preds['test_truths']), np.array(list(zip(Bestmodel_preds['test_pred_prob'], Bestmodel_preds['test_predictions']))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model_load = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model_load.cuda()\n",
    "model_load.load_state_dict(torch.load('model/full_data_read_bert_lr_2e-05_numepoch_5_currepoch2.pth'))\n",
    "model_load.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_with_pred(model):\n",
    "# test set evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    truths = []\n",
    "    pred_prob_lst = []\n",
    "    data_for_example = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data, b_input_mask, labels) in enumerate(dataloaders['test']):\n",
    "            data, b_input_mask, labels = data.to(device), b_input_mask.to(device), labels.to(device)\n",
    "            (loss, outputs) = model(data, \n",
    "                   token_type_ids=None, \n",
    "                   attention_mask=b_input_mask,\n",
    "                   labels=labels)\n",
    "            model.zero_grad()\n",
    "#                 loss is automatically output from the model. So no need to run loss separately\n",
    "#                 loss = loss_fn(outputs, labels)\n",
    "            pred_prob, preds = torch.max(outputs, dim = 1)\n",
    "            predictions += list(preds.cpu().numpy())\n",
    "            pred_prob_lst += list(pred_prob.cpu().numpy())\n",
    "            truths += list(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum()\n",
    "            data_for_example.append(data)\n",
    "\n",
    "        acc = (1.0 * correct / total)\n",
    "#         elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "        print('Test set | Accuracy: {:6.4f}'.format(\n",
    "            acc))\n",
    "    \n",
    "    return {'Model': model, \n",
    "            'test_predictions': predictions, 'test_pred_prob': pred_prob_lst, 'test_truths': truths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bestmodel_preds = test_model_with_pred(model_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(Bestmodel_preds['test_truths'], Bestmodel_preds['test_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr , tpr , thresholds = roc_curve(Bestmodel_preds['test_truths'], Bestmodel_preds['test_predictions'])\n",
    "\n",
    "plt.plot(fpr,tpr) \n",
    "plt.axis([0,1,0,1]) \n",
    "plt.xlabel('False Positive Rate') \n",
    "plt.ylabel('True Positive Rate') \n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skplt.metrics.plot_precision_recall_curve(np.array(Bestmodel_preds['test_truths']), \n",
    "                                          np.array(list(zip(Bestmodel_preds['test_pred_prob'], \n",
    "                                                    Bestmodel_preds['test_predictions']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc_curve(np.array(Bestmodel_preds['test_truths']), np.array(list(zip(Bestmodel_preds['test_pred_prob'], Bestmodel_preds['test_predictions']))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
